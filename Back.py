# -*- coding: utf-8 -*-
"""DS_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ViOaco84kfB9kMYKJL9ROjvirhfi1zU

## Deskripsi Dataset

Dataset *Customer Churn* merupakan kumpulan data yang berisi informasi nasabah dari sebuah bank multinasional anonim. Data ini mencakup berbagai aspek terkait aktivitas keuangan nasabah, seperti saldo rata-rata, jumlah dan frekuensi pembelian, penggunaan uang tunai, batas kredit, serta kebiasaan pembayaran. Setiap baris mewakili satu nasabah dengan karakteristik unik berdasarkan perilaku transaksi mereka. Dataset ini digunakan untuk menganalisis pola perilaku nasabah dan membangun model prediksi yang dapat membantu bank mengidentifikasi nasabah yang berpotensi berhenti atau berpindah ke lembaga keuangan lain (churn). Secara umum, dataset ini menjadi dasar penting dalam penelitian data science untuk memahami faktor-faktor yang memengaruhi loyalitas nasabah di sektor perbankan.

## Fitur Dataset

RowNumber — menunjukkan nomor urut baris data dan tidak memiliki pengaruh terhadap hasil (output).

CustomerId — berisi nilai acak dan tidak berpengaruh terhadap keputusan nasabah untuk meninggalkan bank.

Surname — nama belakang nasabah tidak memiliki dampak terhadap keputusan mereka untuk meninggalkan bank.

CreditScore — dapat mempengaruhi tingkat churn (nasabah keluar), karena nasabah dengan skor kredit yang lebih tinggi cenderung lebih kecil kemungkinannya untuk meninggalkan bank.

Geography — lokasi nasabah dapat mempengaruhi keputusan mereka untuk meninggalkan bank.

Gender — menarik untuk diteliti apakah jenis kelamin berperan dalam keputusan nasabah meninggalkan bank.

Age — sangat relevan, karena nasabah yang lebih tua cenderung lebih setia dan lebih kecil kemungkinan untuk meninggalkan bank dibandingkan yang lebih muda.

Tenure — menunjukkan jumlah tahun nasabah telah menjadi klien bank. Biasanya, nasabah yang lebih lama bergabung lebih loyal dan lebih kecil kemungkinan untuk keluar.

Balance — juga merupakan indikator yang baik untuk churn; orang dengan saldo lebih tinggi di rekeningnya lebih kecil kemungkinannya keluar dibandingkan mereka yang memiliki saldo rendah.

NumOfProducts — menunjukkan jumlah produk yang telah dibeli nasabah melalui bank.

HasCrCard — menunjukkan apakah nasabah memiliki kartu kredit atau tidak. Variabel ini juga relevan, karena mereka yang memiliki kartu kredit cenderung lebih setia pada bank.

IsActiveMember — nasabah yang aktif umumnya lebih kecil kemungkinan untuk meninggalkan bank.

EstimatedSalary — seperti halnya saldo, nasabah dengan gaji lebih rendah cenderung lebih mungkin meninggalkan bank dibandingkan dengan mereka yang berpenghasilan lebih tinggi.

Exited — menunjukkan apakah nasabah meninggalkan bank atau tidak.

Complain — menunjukkan apakah nasabah memiliki keluhan atau tidak.

Satisfaction Score — skor yang diberikan nasabah terkait kepuasan mereka terhadap penyelesaian keluhan.

Card Type — jenis kartu yang dimiliki oleh nasabah.

Points Earned — jumlah poin yang diperoleh nasabah dari penggunaan kartu kredit.

## Import Library
"""



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import sys

from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression  # Baseline
from sklearn.ensemble import RandomForestClassifier  # Model Utama
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc

"""## url dataset = https://drive.google.com/file/d/1UgiiF9nIcgmsIoAhOaeA7hXRkzhjdqeN/view?usp=drive_link


"""

url ='https://drive.google.com/file/d/1UgiiF9nIcgmsIoAhOaeA7hXRkzhjdqeN'

"""## 1. Load dan Lihat Data"""

url ='https://drive.google.com/uc?id=1UgiiF9nIcgmsIoAhOaeA7hXRkzhjdqeN'

df = pd.read_csv(url)
print("\nData Frame sebelum cleaning :")
display(df.head())
df.info()

"""## 2. Data Cleaning"""

# Kita simpan ke variabel baru bernama df_clean
df_clean = df.copy()

# 1. Hapus kolom identitas
df_clean = df_clean.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# 2. Hapus kolom Complain (Penyebab data leakage/curang)
if 'Complain' in df_clean.columns:
    df_clean = df_clean.drop('Complain', axis=1)

print("Data Setelah Cleaning :\n")
display(df_clean.head())
df_clean.info()

"""Melihat Missing Value"""

print(df_clean.isnull().sum())

"""Melihat Data yang Duplikat"""

print(df_clean.duplicated().sum())

"""## 3. Feature Engineering"""

# Buat fitur baru
df_clean['BalanceSalaryRatio'] = df_clean['Balance'] / df_clean['EstimatedSalary']
df_clean['TenureByAge'] = df_clean['Tenure'] / df_clean['Age']

print("Data frame dengan feature baru.")
df_clean.head()

"""Penjelasan Feature Baru : <br>


1.   BalanceSalaryRatio (Rasio Saldo terhadap Gaji): Fitur ini dihitung dengan membagi saldo nasabah (Balance) dengan estimasi gaji mereka (EstimatedSalary). Tujuannya adalah untuk memahami perilaku finansial nasabah. Nasabah dengan rasio tinggi cenderung menjadikan bank ini sebagai tempat tabungan utama mereka (lebih loyal), sedangkan nasabah dengan rasio rendah mungkin hanya menggunakan rekening untuk transaksi sehari-hari atau memiliki akun utama di bank lain.
2.   TenureByAge (Rasio Loyalitas terhadap Umur): Fitur ini dihitung dengan membagi lama berlangganan (Tenure) dengan usia nasabah (Age). Fitur ini menormalisasi loyalitas berdasarkan tahap kehidupan. Nasabah muda yang sudah berlangganan 2 tahun memiliki tingkat loyalitas relatif yang lebih tinggi dibandingkan nasabah tua dengan durasi yang sama. Hal ini membantu model membedakan antara "nasabah baru" dan "nasabah setia" dengan lebih adil.

## 4. Pre-Processing
"""

# 1. Encoding (Mengubah data Teks menjadi Angka)
df_model = pd.get_dummies(df_clean, columns=['Geography', 'Gender', 'Card Type'], drop_first=True)

numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']
categorical_features = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Complain', 'Satisfaction Score', 'Card Type', 'Point Earned']

"""Menampilkan Heatmap"""

# Hitung matriks korelasi dari dataframe final
correlation_matrix = df_model.corr()

# Buat visualisasi heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.title('Heatmap Korelasi Final Data Nasabah Bank', fontsize=18)
plt.show()

"""## Distribusi Data Numerik"""

# DISTRIBUSI FITUR NUMERIK
true_numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance',
                     'NumOfProducts', 'EstimatedSalary',
                     'BalanceSalaryRatio', 'TenureByAge',
                     'Satisfaction Score', 'Point Earned']

# Pastikan hanya ambil kolom yang benar-benar ada
true_numeric_cols = [col for col in true_numeric_cols if col in df.columns]

df[true_numeric_cols].hist(
    figsize=(16, 12),
    bins=30,
    color='#3498db',
    edgecolor='black',
    layout=(3, 3)
)

plt.suptitle('Distribusi Fitur Numerik', fontsize=18, fontweight='bold', y=0.98)
plt.tight_layout()
plt.show()

# DISTRIBUSI KATEGORIKAL
fig, ax = plt.subplots(1, 4, figsize=(20, 5))

sns.countplot(data=df, x='Gender', ax=ax[0], palette='Set2', order=['Male', 'Female'])
ax[0].set_title('Distribusi Gender')

sns.countplot(data=df, x='Geography', ax=ax[1], palette='Set2')
ax[1].set_title('Distribusi Geography')

sns.countplot(data=df, x='Card Type', ax=ax[2], palette='husl')
ax[2].set_title('Distribusi Card Type')

sns.countplot(data=df, x='IsActiveMember', ax=ax[3], palette='Set1')
ax[3].set_title('Distribusi Active Member')
ax[3].set_xticklabels(['Tidak Aktif', 'Aktif'])

plt.tight_layout()
plt.show()

"""Distribusi Churn"""

plt.figure(figsize=(8, 5))
sns.countplot(x=df_clean['Exited'], edgecolor='black')

plt.xticks([0, 1], ['Non Churn', 'Churn'])
plt.title('Jumlah Churn vs Non Churn', fontsize=16, fontweight='bold')
plt.xlabel('Kategori')
plt.ylabel('Jumlah')
plt.show()

churned_count = df[df['Exited'] == 1]['Exited'].count()
not_churned_count = df[df['Exited'] == 0]['Exited'].count()

total = churned_count + not_churned_count

presentase_churn = (churned_count / total) * 100
presentase_tidak_churn = (not_churned_count / total) * 100

print(f'Customer Yang Churn: {churned_count} ({presentase_churn:.2f}%)')
print(f'Customer Yang Tidak Churn: {not_churned_count} ({presentase_tidak_churn:.2f}%)')

"""Persebaran Churn"""

sns.pairplot(df_clean[['CreditScore', 'Age', 'Balance', 'Exited']], hue='Exited', palette='husl')
plt.show()

"""Churn vs Non Churn pada Kredit Score"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df_clean, x='Exited', y='CreditScore', palette='Set1')
plt.xlabel('Churn (Exited)')
plt.ylabel('CreditScore')
plt.title('Churn pada CreditScore')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

credit_score_churn_counts = df.groupby(['Exited'])['CreditScore']
churn_rate = (credit_score_churn_counts.sum() / credit_score_churn_counts.count()).tolist()
print(f'Churn Rate - Not Churned: {churn_rate[0]:.2f}')
print(f'Churn Rate - Churned: {churn_rate[1]:.2f}')

"""Churn vs Non Churn pada tenure"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Exited', y='Tenure', palette='Set1')
plt.xlabel('Churn (Exited)')
plt.ylabel('Tenure')
plt.title('Churn pada Tenure')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

tenure_churn_counts = df.groupby(['Tenure', 'Exited']).size().unstack()
for tenure in df['Tenure'].unique():
    total_customers = tenure_churn_counts.loc[tenure].sum()
    churn_rate = tenure_churn_counts.loc[tenure][1] / total_customers
    print(f'Churn Rate - Tenure {tenure}: {churn_rate:.2%}')

"""Churn vs Non Churn pada umur"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Exited', y='Age', palette='Set1')
plt.xlabel('Churn (Exited)')
plt.ylabel('Age')
plt.title('Churn pada Umur')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

age_churn_counts = df.groupby(['Exited'])['Age']



churn_rate =  df.groupby('Exited')['Age'].mean()
print(f'Rata Rata Usia')
print(f'Not Churned: {churn_rate[0]:.2f}')
print(f'Churned: {churn_rate[1]:.2f}')

"""Churn vs Non Churn pada NumberOfProducts"""

plt.figure(figsize=(8, 4))
sns.boxplot(data=df, x='Exited', y='NumOfProducts', palette='Set1')
plt.xlabel('Churn (Exited)')
plt.ylabel('Num Of Products')
plt.title('Churn pada NumberOfProduct')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

"""Churn vs Non Churn pada balance"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Exited', y='Balance', palette='Set1')
plt.xlabel('Churn (Exited)')
plt.ylabel('Balance')
plt.title('Distribusi churn pada balance')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

"""Churn vs Non Churn pada perkiraan gaji"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Exited', y='EstimatedSalary', palette='Set1')
plt.xlabel('Churn (Exited)')
plt.ylabel('Estimated Salary')
plt.title('Distribusi churn pada perkiraan gaji')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

"""Churn vs Non Churn pada membership"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='IsActiveMember', hue='Exited', palette='Set1')
plt.xlabel('Active Membership')
plt.ylabel('Count')
plt.title('Distribusi churn pada membership')
plt.legend(['Not Churned', 'Churned'])
plt.xticks([0, 1], ['Inactive', 'Active'])
plt.show()

"""Churn vs Non Churn pada gender"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Gender', hue='Exited', palette='Set1')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Churn vs Non Churn pada gender')
plt.legend(['Not Churned', 'Churned'])
plt.xticks([0, 1], ['Female', 'Male'])
plt.show()

gender_churn_counts = df.groupby(['Gender', 'Exited']).size().unstack()

for gender in df['Gender'].unique():
    total_customers = gender_churn_counts.loc[gender].sum()
    churn_rate = gender_churn_counts.loc[gender][1] / total_customers
    print(f'Churn Rate - {gender}: {churn_rate:.1%}')

"""Churn vs Non Churn pada CustomerComplaint"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Complain', hue='Exited', palette='Set1')
plt.xlabel('Customer Complaint')
plt.ylabel('Count')
plt.title('Churn by Customer Complaint')
plt.legend(['Not Churned', 'Churned'])
plt.xticks([0, 1], ['No Complaint', 'Complaint'])
plt.show()

complaint_churn_counts = df.groupby(['Complain', 'Exited']).size().unstack()
for complaint in df['Complain'].unique():
    total_customers = complaint_churn_counts.loc[complaint].sum()
    churn_rate = complaint_churn_counts.loc[complaint][1] / total_customers
    print(f'Churn Rate - Complaint: {complaint} - {churn_rate:.2%}')

"""Churn vs Non Churn pada CardType"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Card Type', hue='Exited', palette='Set1')
plt.xlabel('Card Type')
plt.ylabel('Count')
plt.title('Churn vs Non Churn pada CardType')
plt.legend(['Not Churned', 'Churned'])
plt.show()

card_type_churn_counts = df.groupby(['Card Type', 'Exited']).size().unstack()
for card_type in df['Card Type'].unique():
    total_customers = card_type_churn_counts.loc[card_type].sum()
    churn_rate = card_type_churn_counts.loc[card_type][1] / total_customers
    print(f'Churn Rate - {card_type}: {churn_rate:.2%}')

"""##Splitting Data


"""

# 2. Memisahkan Fitur (X) dan Target (y)
X = df_model.drop('Exited', axis=1) # Data untuk belajar (Semua kecuali jawaban)
y = df_model['Exited']              # Jawaban (Kunci Jawaban)

# 3. Split Data (80% Training, 20% Testing)
# stratify=y memastikan proporsi Churn di data latih dan uji tetap seimbang
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 4. Scaling (Standardisasi Angka)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nData siap untuk modeling!")
print(f"Jumlah Data Latih: {X_train.shape[0]} baris")
print(f"Jumlah Data Uji: {X_test.shape[0]} baris")

"""##Modeling Logistic Regresion"""

# --- LANGKAH 5: MODELING BASELINE (LOGISTIC REGRESSION) ---
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("=== HASIL MODEL 1: LOGISTIC REGRESSION (BASELINE) ===")

# Inisialisasi Model
log_model = LogisticRegression(random_state=42)

# Latih Model (Pakai data yang sudah di-scaling!)
log_model.fit(X_train_scaled, y_train)

# Prediksi Data Uji
y_pred_log = log_model.predict(X_test_scaled)

# Evaluasi
print("Akurasi:", accuracy_score(y_test, y_pred_log))
print("\nLaporan Klasifikasi:")
print(classification_report(y_test, y_pred_log))

"""## Model Random Forest"""

# --- LANGKAH 6: MODELING UTAMA (RANDOM FOREST) ---
from sklearn.ensemble import RandomForestClassifier

print("=== HASIL MODEL 2: RANDOM FOREST (MAIN MODEL) ===")

# Inisialisasi Model
# class_weight='balanced' membantu model lebih peka terhadap nasabah yang Churn (minoritas)
rf_model = RandomForestClassifier(n_estimators=100,
                                  random_state=42,
                                  class_weight='balanced')

# Latih Model (Random Forest bisa pakai data asli X_train tanpa scaling)
rf_model.fit(X_train, y_train)

# Prediksi Data Uji
y_pred_rf = rf_model.predict(X_test)

# Evaluasi
print("Akurasi:", accuracy_score(y_test, y_pred_rf))
print("\nLaporan Klasifikasi:")
print(classification_report(y_test, y_pred_rf))

"""### Confusion Matrix"""

# --- LANGKAH 7: VISUALISASI CONFUSION MATRIX ---
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Siapkan kanvas (frame) gambar dengan 2 kolom (Kiri & Kanan)
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# 1. Plot Baseline (Logistic Regression) - Kiri
# PENTING: LogReg pakai data yang di-SCALING (X_test_scaled)
ConfusionMatrixDisplay.from_estimator(log_model, X_test_scaled, y_test, ax=ax[0], cmap='Blues')
ax[0].set_title('Baseline: Logistic Regression')

# 2. Plot Main Model (Random Forest) - Kanan
# PENTING: RF pakai data ASLI (X_test) karena dia tidak butuh scaling
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, ax=ax[1], cmap='Greens')
ax[1].set_title('Main Model: Random Forest')

plt.tight_layout()
plt.show()

"""## Precission"""

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Siapkan kanvas
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# 1. Plot Baseline (LogReg) - Pakai Scaling!
# normalize='true' artinya: Persentase dihitung per Baris (Baris Churn & Baris Tidak Churn)
# values_format='.1%' artinya: Tampilkan sebagai persen dengan 1 angka di belakang koma (misal 85.5%)
ConfusionMatrixDisplay.from_estimator(log_model, X_test_scaled, y_test, ax=ax[0],
                                      cmap='Blues', normalize='pred', values_format='.1%')
ax[0].set_title('Baseline: Logistic Regression (Dalam %)')

# 2. Plot Main Model (Random Forest) - Pakai Data Asli!
# Ubah normalize='true' menjadi normalize='pred'
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, ax=ax[1],
                                      cmap='Greens', normalize='pred', values_format='.1%')

ax[1].set_title('Main Model: Random Forest (Dalam % Prediksi)')

fig.suptitle('Perbandingan berdasarkan Precision', fontsize=16, y=1.05)
plt.tight_layout()
plt.show()

print("""
CARA BACA PERSENTASE (Fokus ke Baris Bawah '1'):
- Lihat kotak KANAN BAWAH (True Positive).
- Jika angkanya misal 60%, artinya Model berhasil menangkap 60% dari seluruh nasabah yang benar-benar Churn.
- Bandingkan Kiri vs Kanan. Random Forest harusnya punya persentase lebih tinggi di kotak ini.
""")

"""## Recall"""

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt


# Siapkan kanvas
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# 1. Plot Baseline (LogReg) - Pakai Scaling!
# normalize='true' artinya: Persentase dihitung per Baris (Baris Churn & Baris Tidak Churn)
# values_format='.1%' artinya: Tampilkan sebagai persen dengan 1 angka di belakang koma (misal 85.5%)
ConfusionMatrixDisplay.from_estimator(log_model, X_test_scaled, y_test, ax=ax[0],
                                      cmap='Blues', normalize='true', values_format='.1%')
ax[0].set_title('Baseline: Logistic Regression (Dalam %)')

# 2. Plot Main Model (Random Forest) - Pakai Data Asli!
# Ubah normalize='true' menjadi normalize='pred'
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, ax=ax[1],
                                      cmap='Greens', normalize='true', values_format='.1%')

ax[1].set_title('Main Model: Random Forest (Dalam % Prediksi)')
fig.suptitle('Perbandingan berdasarkan Recall', fontsize=16, y=1.05)

plt.tight_layout()
plt.show()

print("""
CARA BACA PERSENTASE (Fokus ke Baris Bawah '1'):
- Lihat kotak KANAN BAWAH (True Positive).
- Jika angkanya misal 60%, artinya Model berhasil menangkap 60% dari seluruh nasabah yang benar-benar Churn.
- Bandingkan Kiri vs Kanan. Random Forest harusnya punya persentase lebih tinggi di kotak ini.
""")

"""## ROC

"""

# --- LANGKAH 8: PERBANDINGAN KURVA ROC ---
from sklearn.metrics import roc_curve, auc

# 1. Hitung Probabilitas (Peluang)
# Model tidak cuma tebak Ya/Tidak, tapi tebak "% Kemungkinan Churn"
y_prob_log = log_model.predict_proba(X_test_scaled)[:, 1] # Ambil peluang kelas 1 (Churn)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]

# 2. Hitung FPR (False Positive) dan TPR (True Positive)
fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_log)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)

# 3. Hitung Luas Area (AUC Score)
auc_log = auc(fpr_log, tpr_log)
auc_rf = auc(fpr_rf, tpr_rf)

# 4. Plotting
plt.figure(figsize=(10, 6))

# Garis Logistik (Putus-putus)
plt.plot(fpr_log, tpr_log, linestyle='--', color='blue', label=f'Logistic Regression (AUC = {auc_log:.2f})')

# Garis Random Forest (Tebal)
plt.plot(fpr_rf, tpr_rf, linewidth=3, color='green', label=f'Random Forest (AUC = {auc_rf:.2f})')

# Garis Tebakan Ngawur (50:50)
plt.plot([0, 1], [0, 1], 'k--', alpha=0.3)

plt.title('Kurva ROC: Perbandingan Model', fontsize=14)
plt.xlabel('False Positive Rate (Kesalahan Deteksi)')
plt.ylabel('True Positive Rate (Sensitivitas Deteksi)')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.show()

"""## Feature Importance"""

# --- LANGKAH 9: FEATURE IMPORTANCE (INSIGHT) ---
import pandas as pd

# Ambil data tingkat kepentingan fitur dari Random Forest
importances = rf_model.feature_importances_
feature_names = X.columns

# Buat Series dan urutkan dari yang terbesar
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

# Plot 10 Fitur Teratas
plt.figure(figsize=(10, 8))
# Pakai warna Teal biar estetik
feat_imp.head(10).plot(kind='barh', color='#2a9d8f')

plt.title('10 Faktor Utama Penyebab Nasabah Churn (Menurut Random Forest)', fontsize=14)
plt.xlabel('Tingkat Pengaruh (Importance)')
plt.gca().invert_yaxis() # Membalik urutan biar ranking 1 ada di atas
plt.grid(axis='x', alpha=0.3)
plt.show()

# Print teks kesimpulan otomatis
print("INSIGHT BISNIS:")
print(f"1. Faktor paling dominan adalah: {feat_imp.index[0]}")
print(f"2. Faktor kedua adalah: {feat_imp.index[1]}")
print(f"3. Faktor ketiga adalah: {feat_imp.index[2]}")


# Streamlit App: Bank Customer Churn Explorer + Modeling
# Save as `app_streamlit_churn.py` and run with: `streamlit run app_streamlit_churn.py`

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

st.set_page_config(layout="wide", page_title="Customer Churn App")

# ---------------------- Helper functions (cached) ----------------------
@st.cache_data
def load_data_from_url(url: str):
    # support Google Drive "uc?id=" link
    try:
        return pd.read_csv(url)
    except Exception as e:
        st.error(f"Gagal load data: {e}")
        return pd.DataFrame()

@st.cache_data
def preprocess(df: pd.DataFrame):
    df_clean = df.copy()
    # drop identity columns if exist
    for col in ['RowNumber', 'CustomerId', 'Surname']:
        if col in df_clean.columns:
            df_clean = df_clean.drop(col, axis=1)
    # drop Complain if present (as in notebook)
    if 'Complain' in df_clean.columns:
        df_clean = df_clean.drop('Complain', axis=1)

    # New features
    if 'Balance' in df_clean.columns and 'EstimatedSalary' in df_clean.columns:
        df_clean['BalanceSalaryRatio'] = df_clean['Balance'] / (df_clean['EstimatedSalary'] + 1e-9)
    if 'Tenure' in df_clean.columns and 'Age' in df_clean.columns:
        df_clean['TenureByAge'] = df_clean['Tenure'] / (df_clean['Age'] + 1e-9)

    # One-hot for categorical features used in model
    df_model = pd.get_dummies(df_clean, columns=[col for col in ['Geography','Gender','Card Type'] if col in df_clean.columns], drop_first=True)
    return df_clean, df_model

@st.cache_data
def split_scale(df_model: pd.DataFrame, target='Exited'):
    X = df_model.drop(target, axis=1)
    y = df_model[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test, scaler

@st.cache_data
def train_models(X_train, X_train_scaled, y_train):
    log_model = LogisticRegression(random_state=42, max_iter=1000)
    log_model.fit(X_train_scaled, y_train)

    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    rf_model.fit(X_train, y_train)
    return log_model, rf_model

# ---------------------- Sidebar - Data source & options ----------------------
st.sidebar.title("Kontrol")
data_source = st.sidebar.radio("Pilih sumber data:", ("Upload CSV", "Google Drive URL (uc?id=)", "Use sample (included)") )

if data_source == 'Upload CSV':
    uploaded_file = st.sidebar.file_uploader("Unggah file CSV", type=['csv'])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.DataFrame()
elif data_source == 'Google Drive URL (uc?id=)':
    url = st.sidebar.text_input('Masukkan Google Drive URL (format: https://drive.google.com/uc?id=FILEID)')
    if url:
        df = load_data_from_url(url)
    else:
        df = pd.DataFrame()
else:
    # small sample constructed from commonly used churn dataset structure (fallback)
    st.sidebar.markdown("Menggunakan sample dataset bawaan.")
    sample_url = 'https://raw.githubusercontent.com/azharimmf/Customer-Churn-Analytics/master/data/BankChurners.csv'
    df = load_data_from_url(sample_url)

st.title("Bank Customer Churn — Explorer & Modeling")

if df.empty:
    st.info("Silakan unggah file CSV atau masukkan URL Google Drive di sidebar untuk memulai.")
    st.stop()

# Show raw data
with st.expander("Lihat data mentah (head)"):
    st.dataframe(df.head())
    st.write(df.shape)

# Preprocess
df_clean, df_model = preprocess(df)

col1, col2 = st.columns([2, 1])
with col1:
    st.subheader("Ringkasan Data Setelah Cleaning & Feature Engineering")
    st.dataframe(df_clean.head())
with col2:
    st.subheader("Statistik Deskriptif")
    st.write(df_clean.describe().T)

# ---------------------- EDA ----------------------
st.markdown("---")
st.header("Exploratory Data Analysis (EDA)")

# Correlation heatmap
if st.checkbox('Tampilkan heatmap korelasi'):
    fig, ax = plt.subplots(figsize=(12, 8))
    corr = df_model.corr()
    sns.heatmap(corr, annot=False, cmap='coolwarm', ax=ax)
    ax.set_title('Heatmap Korelasi')
    st.pyplot(fig)

# Distribution plots selection
numeric_cols = df_clean.select_dtypes(include=np.number).columns.tolist()
sel_num = st.multiselect('Pilih kolom numerik untuk plot distribusi', numeric_cols, default=numeric_cols[:4])
if sel_num:
    fig = plt.figure(figsize=(12, 4))
    for i, col in enumerate(sel_num):
        ax = fig.add_subplot(1, len(sel_num), i+1)
        sns.histplot(df_clean[col].dropna(), kde=True, ax=ax)
        ax.set_title(col)
    st.pyplot(fig)

# Churn counts
st.subheader('Distribusi Churn')
fig, ax = plt.subplots()
sns.countplot(x=df_clean['Exited'], ax=ax)
ax.set_xticklabels(['Non Churn','Churn'])
st.pyplot(fig)

# ---------------------- Modeling ----------------------
st.markdown("---")
st.header("Modeling & Evaluation")

X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test, scaler = split_scale(df_model)
log_model, rf_model = train_models(X_train, X_train_scaled, y_train)

model_choice = st.selectbox('Pilih model untuk dievaluasi:', ('Logistic Regression (baseline)', 'Random Forest (main)'))

if st.button('Tampilkan evaluasi model'):
    if model_choice.startswith('Logistic'):
        y_pred = log_model.predict(X_test_scaled)
        y_prob = log_model.predict_proba(X_test_scaled)[:,1]
    else:
        y_pred = rf_model.predict(X_test)
        y_prob = rf_model.predict_proba(X_test)[:,1]

    st.subheader('Akurasi & Classification Report')
    st.write('Akurasi:', accuracy_score(y_test, y_pred))
    st.text(classification_report(y_test, y_pred))

    # Confusion Matrix
    fig, ax = plt.subplots()
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', ax=ax)
    ax.set_xlabel('Prediksi')
    ax.set_ylabel('Aktual')
    ax.set_title('Confusion Matrix')
    st.pyplot(fig)

    # ROC
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    ax.plot([0,1],[0,1],'--', alpha=0.3)
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('ROC Curve')
    ax.legend()
    st.pyplot(fig)

# Feature importance (Random Forest)
if st.checkbox('Tampilkan feature importance (Random Forest)'):
    importances = rf_model.feature_importances_
    feat_names = X_train.columns
    feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=True)
    fig, ax = plt.subplots(figsize=(8, 10))
    feat_imp.tail(15).plot(kind='barh', ax=ax)
    ax.set_title('Feature Importance (Random Forest)')
    st.pyplot(fig)

# ---------------------- Interactive Predictions ----------------------
st.markdown('---')
st.header('Prediksi Interaktif')

st.write('Masukkan fitur manual atau pilih baris dari dataset untuk melihat prediksi model.')

use_row = st.checkbox('Pilih baris dari data (preview) untuk prediksi')

if use_row:
    idx = st.number_input('Index (0..n-1) dari dataset yang dipilih', min_value=0, max_value=max(0, len(df_model)-1), value=0)
    input_row = df_model.drop('Exited', axis=1).iloc[[idx]]
else:
    # create simple manual inputs for the most important numeric features if exist
    input_row = {}
    for col in ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','BalanceSalaryRatio','TenureByAge']:
        if col in df_model.columns:
            input_row[col] = st.number_input(col, value=float(df_model[col].median()))
    # for missing categorical dummies just set zeros
    input_row = pd.DataFrame([input_row])
    # add any missing columns with 0
    for c in df_model.drop('Exited', axis=1).columns:
        if c not in input_row.columns:
            input_row[c] = 0.0
    input_row = input_row[df_model.drop('Exited', axis=1).columns]

st.subheader('Fitur untuk Prediksi')
st.dataframe(input_row)

pred_model = st.selectbox('Pilih model untuk prediksi:', ('Random Forest', 'Logistic Regression'))
if st.button('Predict'):
    if pred_model == 'Logistic Regression':
        X_in = scaler.transform(input_row)
        prob = log_model.predict_proba(X_in)[0,1]
        pred = log_model.predict(X_in)[0]
    else:
        # RF expects original scaling
        prob = rf_model.predict_proba(input_row)[0,1]
        pred = rf_model.predict(input_row)[0]
    st.metric('Probabilitas Churn', f"{prob:.2%}")
    st.write('Prediksi:', 'Churn' if pred==1 else 'Non Churn')

st.markdown('---')
st.write('Catatan: Aplikasi ini meniru alur notebook Anda: cleaning, feature engineering, EDA, training baseline & RF, evaluasi, dan prediksi interaktif.')

# End
